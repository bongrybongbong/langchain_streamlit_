{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.agents import agent_types, initialize_agent, load_tools\n",
    "from langchain.llms import openai\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "#사용할 도구 로드\n",
    "tools = load_tools([\"wikipedia\",\"llm-math\"], llm=llm)\n",
    "\n",
    "#에이전트 초기화 방법 사용해서 에이전트 생성\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    #어디로 가야할 지 생각을 먼저해서 파악을 하고 행동을 취한다\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    #추론 과정에서 단계별로 진행하고 싶을 경우 -> 수행 중인 내부 단계를 파악하기 위해 모든 함수에서 사용할 수 잇는 변수\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent.run(\"일론머스크는 언제 태어났고 2023년에 그의 나이는 몇 살이야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "#사용할 도구 로드\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    #어디로 가야할 지 생각을 먼저해서 파악을 하고 행동을 취한다\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    #추론 과정에서 단계별로 진행하고 싶을 경우 -> 수행 중인 내부 단계를 파악하기 위해 모든 함수에서 사용할 수 잇는 변수\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent.run(\"한국의 2022년 gdp는 얼마야? 그리고 거기다가 10을 더한 값은 얼마니?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import agent_types, initialize_agent, load_tools\n",
    "from langchain.llms import openai\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.7)\n",
    "# sequential chain\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "Prompt_tempate_name = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template = \"I WANNA OPEN A RESTAURENT FOR {cuisine} FOOD. SUGGEST A FENCY NAME FOR THEIS.\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=Prompt_tempate_name, output_key=\"restaurant_name\")\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "Prompt_tempate_items = PromptTemplate(\n",
    "    input_variables= ['restaurant_name'],\n",
    "    template = \"SUGGEST SOME MENU ITEMS FOR {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "Food_items_chain = LLMChain(llm = llm, prompt= Prompt_tempate_items, output_key= \"menu_items\")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = LLMChain(llm=llm,prompt=Prompt_tempate_name, memory =memory)\n",
    "name = chain.run(\"korean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#개방형 ai 링크체인은 매우 간단\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대화 버퍼 메모리의 문제는 끝없이 커진다 -> 한쌍의 q&a가 다 개별취급 -> 한명과 한 주제에 대해서 여러 이야기를 나눠도 다 따로 취급 -> 한 토큰당 비용 청구함. 계속 올아갈 듯\n",
    "#최적화를 위해서는 버퍼사이즈 제한해야함"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
